{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Wheat Detection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D1fZIGUI_ctT",
        "colab_type": "text"
      },
      "source": [
        "Wheat Detection - Kaggle Competition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jAOwAME2BMjN",
        "colab_type": "text"
      },
      "source": [
        "Load Yolov5"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JI_t-MNf_aSp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "1b96b32e-8e0c-4156-d06c-0d72dfe7f371"
      },
      "source": [
        "!git clone https://github.com/samthomp/yolov5 # clone repo with customized wheat edits"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'yolov5'...\n",
            "remote: Enumerating objects: 30, done.\u001b[K\n",
            "remote: Counting objects: 100% (30/30), done.\u001b[K\n",
            "remote: Compressing objects: 100% (27/27), done.\u001b[K\n",
            "remote: Total 834 (delta 14), reused 8 (delta 3), pack-reused 804\u001b[K\n",
            "Receiving objects: 100% (834/834), 3.39 MiB | 760.00 KiB/s, done.\n",
            "Resolving deltas: 100% (544/544), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vywiV5yU-Um-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "739d12e7-e495-4149-c7c4-63a07cb58088"
      },
      "source": [
        "%cd yolov5\n",
        "!pip install -U -r requirements.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/yolov5\n",
            "Collecting git+https://github.com/cocodataset/cocoapi.git#subdirectory=PythonAPI (from -r requirements.txt (line 13))\n",
            "  Cloning https://github.com/cocodataset/cocoapi.git to /tmp/pip-req-build-9znpddga\n",
            "  Running command git clone -q https://github.com/cocodataset/cocoapi.git /tmp/pip-req-build-9znpddga\n",
            "Requirement already up-to-date: Cython in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 2)) (0.29.20)\n",
            "Collecting numpy==1.17\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/b9/bda9781f0a74b90ebd2e046fde1196182900bd4a8e1ea503d3ffebc50e7c/numpy-1.17.0-cp36-cp36m-manylinux1_x86_64.whl (20.4MB)\n",
            "\u001b[K     |████████████████████████████████| 20.4MB 7.9MB/s \n",
            "\u001b[?25hCollecting opencv-python\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/c2/e9cf54ae5b1102020ef895866a67cb2e1aef72f16dd1fde5b5fb1495ad9c/opencv_python-4.2.0.34-cp36-cp36m-manylinux1_x86_64.whl (28.2MB)\n",
            "\u001b[K     |████████████████████████████████| 28.2MB 78kB/s \n",
            "\u001b[?25hRequirement already up-to-date: torch>=1.4 in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 5)) (1.5.1+cu101)\n",
            "Requirement already up-to-date: matplotlib in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 6)) (3.2.2)\n",
            "Collecting pillow\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e0/50/8e78e6f62ffa50d6ca95c281d5a2819bef66d023ac1b723e253de5bda9c5/Pillow-7.1.2-cp36-cp36m-manylinux1_x86_64.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 51.8MB/s \n",
            "\u001b[?25hRequirement already up-to-date: tensorboard in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 8)) (2.2.2)\n",
            "Collecting PyYAML>=5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/64/c2/b80047c7ac2478f9501676c988a5411ed5572f35d1beff9cae07d321512c/PyYAML-5.3.1.tar.gz (269kB)\n",
            "\u001b[K     |████████████████████████████████| 276kB 51.3MB/s \n",
            "\u001b[?25hRequirement already up-to-date: torchvision in /usr/local/lib/python3.6/dist-packages (from -r requirements.txt (line 10)) (0.6.1+cu101)\n",
            "Collecting scipy\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/06/20/d4410683e4d416a11ebc60138f6d925f571ffcfcc3794baf78fff982c98d/scipy-1.5.0-cp36-cp36m-manylinux1_x86_64.whl (25.9MB)\n",
            "\u001b[K     |████████████████████████████████| 25.9MB 80kB/s \n",
            "\u001b[?25hCollecting tqdm\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/76/4697ce203a3d42b2ead61127b35e5fcc26bba9a35c03b32a2bd342a4c869/tqdm-4.46.1-py2.py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 11.2MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: setuptools>=18.0 in /usr/local/lib/python3.6/dist-packages (from pycocotools==2.0->-r requirements.txt (line 13)) (47.3.1)\n",
            "Requirement already satisfied, skipping upgrade: future in /usr/local/lib/python3.6/dist-packages (from torch>=1.4->-r requirements.txt (line 5)) (0.16.0)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 6)) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 6)) (1.2.0)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 6)) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib->-r requirements.txt (line 6)) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r requirements.txt (line 8)) (3.2.2)\n",
            "Requirement already satisfied, skipping upgrade: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r requirements.txt (line 8)) (1.17.2)\n",
            "Requirement already satisfied, skipping upgrade: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r requirements.txt (line 8)) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r requirements.txt (line 8)) (0.9.0)\n",
            "Requirement already satisfied, skipping upgrade: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r requirements.txt (line 8)) (1.12.0)\n",
            "Requirement already satisfied, skipping upgrade: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r requirements.txt (line 8)) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r requirements.txt (line 8)) (0.34.2)\n",
            "Requirement already satisfied, skipping upgrade: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r requirements.txt (line 8)) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r requirements.txt (line 8)) (1.30.0)\n",
            "Requirement already satisfied, skipping upgrade: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r requirements.txt (line 8)) (1.6.0.post3)\n",
            "Requirement already satisfied, skipping upgrade: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard->-r requirements.txt (line 8)) (3.10.0)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard->-r requirements.txt (line 8)) (1.6.1)\n",
            "Requirement already satisfied, skipping upgrade: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->-r requirements.txt (line 8)) (4.1.0)\n",
            "Requirement already satisfied, skipping upgrade: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->-r requirements.txt (line 8)) (4.6)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard->-r requirements.txt (line 8)) (0.2.8)\n",
            "Requirement already satisfied, skipping upgrade: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r requirements.txt (line 8)) (1.3.0)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 8)) (2.9)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 8)) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 8)) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard->-r requirements.txt (line 8)) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard->-r requirements.txt (line 8)) (3.1.0)\n",
            "Requirement already satisfied, skipping upgrade: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard->-r requirements.txt (line 8)) (0.4.8)\n",
            "Requirement already satisfied, skipping upgrade: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard->-r requirements.txt (line 8)) (3.1.0)\n",
            "Building wheels for collected packages: PyYAML, pycocotools\n",
            "  Building wheel for PyYAML (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyYAML: filename=PyYAML-5.3.1-cp36-cp36m-linux_x86_64.whl size=44621 sha256=e8352ce73ae351bb0b9051006a1d8241294f6569ebcfedb840c374f737ef35a4\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd\n",
            "  Building wheel for pycocotools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pycocotools: filename=pycocotools-2.0-cp36-cp36m-linux_x86_64.whl size=266985 sha256=07df8b0b17c2834d22a753fc610378e6bd74a88ec5c2fa5f26d51edcdc9da7ed\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-dczaxiq_/wheels/90/51/41/646daf401c3bc408ff10de34ec76587a9b3ebfac8d21ca5c3a\n",
            "Successfully built PyYAML pycocotools\n",
            "\u001b[31mERROR: tensorflow 2.2.0 has requirement scipy==1.4.1; python_version >= \"3\", but you'll have scipy 1.5.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpy, opencv-python, pillow, PyYAML, scipy, tqdm, pycocotools\n",
            "  Found existing installation: numpy 1.18.5\n",
            "    Uninstalling numpy-1.18.5:\n",
            "      Successfully uninstalled numpy-1.18.5\n",
            "  Found existing installation: opencv-python 4.1.2.30\n",
            "    Uninstalling opencv-python-4.1.2.30:\n",
            "      Successfully uninstalled opencv-python-4.1.2.30\n",
            "  Found existing installation: Pillow 7.0.0\n",
            "    Uninstalling Pillow-7.0.0:\n",
            "      Successfully uninstalled Pillow-7.0.0\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Found existing installation: scipy 1.4.1\n",
            "    Uninstalling scipy-1.4.1:\n",
            "      Successfully uninstalled scipy-1.4.1\n",
            "  Found existing installation: tqdm 4.41.1\n",
            "    Uninstalling tqdm-4.41.1:\n",
            "      Successfully uninstalled tqdm-4.41.1\n",
            "  Found existing installation: pycocotools 2.0.1\n",
            "    Uninstalling pycocotools-2.0.1:\n",
            "      Successfully uninstalled pycocotools-2.0.1\n",
            "Successfully installed PyYAML-5.3.1 numpy-1.17.0 opencv-python-4.2.0.34 pillow-7.1.2 pycocotools-2.0 scipy-1.5.0 tqdm-4.46.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "numpy",
                  "tqdm"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4TCYB6ISOyGb",
        "colab_type": "text"
      },
      "source": [
        "Split CSV into multiple Data Files"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FGRL5afoDl9q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "106edd34-93ca-48d6-9ab0-2fbc0e88ae43"
      },
      "source": [
        "# download dataset\n",
        "!python3 -c \"from yolov5.utils.google_utils import gdrive_download; gdrive_download('1rRTfBlMx_3DDXPA9hfcw9EAR6PLQi2d3','wheat.zip')\" \n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100   408    0   408    0     0   2443      0 --:--:-- --:--:-- --:--:--  2457\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
            "100  607M    0  607M    0     0  74.6M      0 --:--:--  0:00:08 --:--:-- 92.4M\n",
            "Downloading https://drive.google.com/uc?export=download&id=1rRTfBlMx_3DDXPA9hfcw9EAR6PLQi2d3 as wheat.zip... unzipping... Done (13.4s)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m2xIb2U7O1ve",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ftcXjpdhFjU5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# file and folder packages\n",
        "import glob\n",
        "import os\n",
        "from os import path\n",
        "\n",
        "# image manipulation related imports\n",
        "import shutil\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from skimage import data\n",
        "from skimage import color\n",
        "from skimage import img_as_float"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n9dHgh3UY-Lf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import math\n",
        "import random\n",
        "import subprocess\n",
        "import time\n",
        "from copy import copy\n",
        "from pathlib import Path\n",
        "from sys import platform\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import yaml\n",
        "from scipy.signal import butter, filtfilt\n",
        "from tqdm import tqdm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnRg__E1ZDOd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e97590e1-351e-45e8-e8b3-126ab4240a58"
      },
      "source": [
        "%cd wheat\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/wheat\n",
            "images\tlabels\ttrain.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQJT06KlZBeu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def resize_img (img):\n",
        "    \n",
        "    scale_percent = 0.5 # percent of original size\n",
        "    width = int(img.shape[1] * scale_percent)\n",
        "    height = int(img.shape[0] * scale_percent)\n",
        "    dim = (width, height)\n",
        "    \n",
        "    resized = cv2.resize(img, dim, interpolation = cv2.INTER_AREA)\n",
        "    \n",
        "    return resized"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTCS0LSWZKEW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "img_sze = 512\n",
        "\n",
        "# add some noise to training data\n",
        "def gauss_blur(image, level):\n",
        "    return cv2.blur(image, (level * 2 + 1, level * 2 + 1))\n",
        "\n",
        "def gauss_noise(image):\n",
        "    \n",
        "    for i in range(image.shape[2]):\n",
        "        c = image[:, :, i]\n",
        "        diff = 255 - c.max();\n",
        "        noise = np.random.normal(0, np.random.randint(1, 6), c.shape)\n",
        "        noise = (noise - noise.min()) / (noise.max() - noise.min())\n",
        "        noise = diff * noise\n",
        "        image[:, :, i] = c + noise.astype(np.uint8)\n",
        "        \n",
        "    return image\n",
        "\n",
        "def constrast_img (image):\n",
        "    \n",
        "    alpha = 1.15 # Simple contrast control 1.0-3.0]\n",
        "    beta = 1    # Simple brightness control  [0-100]\n",
        "    return cv2.convertScaleAbs(image, alpha=alpha, beta=beta)     \n",
        "\n",
        "def green_scale (image):\n",
        "\n",
        "    filter_img  = np.full((img_sze, img_sze, 3), (143, 188, 143), np.uint8)\n",
        "    return cv2.addWeighted(image, 0.8, filter_img, 0.2, 0)\n",
        "    \n",
        "def yellow_scale  (image):\n",
        "    \n",
        "    filter_img  = np.full((img_sze, img_sze, 3), (250, 250, 205), np.uint8)\n",
        "    return cv2.addWeighted(image, 0.85, filter_img, 0.15, 0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5EsYmAsZHj2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Apply combinations of random transformations to images\n",
        "def transform_image(img, select):\n",
        "        \n",
        "    # recolor & tint\n",
        "    if (select[0] > 0):        \n",
        "        img = constrast_img (img)\n",
        "    if (select[1] > 0) :\n",
        "        img = green_scale (img)        \n",
        "    if (select[2] > 0):\n",
        "        img = yellow_scale (img)                           \n",
        "    \n",
        "    # blur & noise\n",
        "    if (select[4] > 0):\n",
        "        img = gauss_blur(img, 2)\n",
        "    if (select[5]) > 0: \n",
        "        img = gauss_noise(img)\n",
        "        \n",
        "    if (select[3] > 0):\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "        \n",
        "    return img"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwaEnWzCru24",
        "colab_type": "text"
      },
      "source": [
        "## make sure to rename the preprocess and preprocess-test folder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IpSa5pxYZOit",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "039cd2cc-d935-490b-f494-23e080a3117b"
      },
      "source": [
        "# Split data randomly into training and test set\n",
        "def preprocess_trainingdata ():\n",
        "    \n",
        "    n = 700    \n",
        "    count = 0\n",
        "    np.random.seed(42)\n",
        "    \n",
        "    all_imgs = glob.glob(\"images/preprocess/*.jpg\")\n",
        "    all_imgs = [i.split(\"/\")[-1].replace(\".jpg\", \"\") for i in all_imgs]\n",
        "    np.random.shuffle(all_imgs)\n",
        "    test_set = all_imgs #[-n:]\n",
        "    \n",
        "    for image_names in test_set:\n",
        "                \n",
        "        select = np.random.randint(2, size=(3422, 6))\n",
        "        \n",
        "        # read the pre-process images, transform then and then save them to the train\n",
        "        img_filename = \"images/preprocess/{}.jpg\".format(image_names)\n",
        "        img = cv2.imread(img_filename, cv2.IMREAD_UNCHANGED)\n",
        "        \n",
        "        # resize the image\n",
        "        img = resize_img (img)\n",
        "        \n",
        "        #  save resized original\n",
        "        img_filename = \"images/train/{}-0.jpg\".format(image_names)\n",
        "        cv2.imwrite(img_filename, img)        \n",
        "                \n",
        "        img_r = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n",
        "        img_filename = \"images/train/{}-r.jpg\".format(image_names)\n",
        "        cv2.imwrite(img_filename, img_r)\n",
        "        \n",
        "        # transform resized image\n",
        "        img = transform_image(img, select[count]) \n",
        "        \n",
        "        # rotate image 180 degrees\n",
        "        img = cv2.rotate(img, cv2.ROTATE_180)\n",
        "        img_filename = \"images/train/{}-i.jpg\".format(image_names)\n",
        "        cv2.imwrite(img_filename, img)\n",
        "                        \n",
        "        count = count + 1\n",
        "        \n",
        "    return count\n",
        "\n",
        "preprocess_trainingdata ()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3422"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YfbPvVm8ZTm-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "727a53fc-78db-427d-e42c-3f5b6d9f4bca"
      },
      "source": [
        "# Add features to test images\n",
        "def preprocess_testdata ():\n",
        "    \n",
        "    #n = 600\n",
        "    count = 0\n",
        "    np.random.seed(42)\n",
        "        \n",
        "    all_imgs = glob.glob(\"images/preprocess-test/*.jpg\")\n",
        "    all_imgs = [i.split(\"/\")[-1].replace(\".jpg\", \"\") for i in all_imgs]\n",
        "\n",
        "    # copy the original images before the transformation\n",
        "    for image_names in all_imgs:        \n",
        "        shutil.copy2(\"images/preprocess-test/{}.jpg\".format(image_names), 'images/test/')\n",
        "    \n",
        "    for image_names in all_imgs:\n",
        "        \n",
        "        # save the transformed images in the train folder\n",
        "        img_filename = \"images/test/{}.jpg\".format(image_names)\n",
        "        img = cv2.imread(img_filename, cv2.IMREAD_UNCHANGED)\n",
        "        count = count + 1\n",
        "                \n",
        "        #img = transform_image(img)           \n",
        "        img = resize_img (img)\n",
        "        #img = constrast_img(img)        \n",
        "        cv2.imwrite(img_filename, img) \n",
        "        \n",
        "    return True\n",
        "\n",
        "preprocess_testdata ()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "utxjGuypZYu-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# test different filters to bring near test set closer to the baseline\n",
        "def coord_rotate(x, y, xm, ym, a):\n",
        "    \n",
        "    cos = math.cos\n",
        "    sin = math.sin\n",
        "\n",
        "    #a = a * math.pi / 180 #onvert to radians because that is what python likes\n",
        "\n",
        "    # Subtract midpoints, so that midpoint is translated to origin\n",
        "    # and add it in the end again\n",
        "    xr = (x - xm) * cos(a) - (y - ym) * sin(a)   + xm\n",
        "    yr = (x - xm) * sin(a) + (y - ym) * cos(a)   + ym\n",
        "\n",
        "    return [xr, yr]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uCWOTQFZcUF",
        "colab_type": "text"
      },
      "source": [
        "Label Processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_xKyGdi_Zbue",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "boxes_df = pd.read_csv('train.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OdYs11YAZe0G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def calculate_inverted (yolo_box):\n",
        "    \n",
        "    xarr = yolo_box.strip('[]').split(\",\") #.astype(float)\n",
        "    xarr = np.array(xarr)\n",
        "    bbox = xarr.astype(float)\n",
        "    \n",
        "    new_coords = coord_rotate(bbox[0], bbox[1], 0.5, 0.5, math.radians(180))\n",
        "    bbox[0] = new_coords[0]\n",
        "    bbox[1] = new_coords[1]\n",
        "         \n",
        "    return str([bbox[0], bbox[1], bbox[2], bbox[3]])\n",
        "\n",
        "## rotate yolo points\n",
        "def calculate_rotated (yolo_box):\n",
        "    \n",
        "    xarr = yolo_box.strip('[]').split(\",\") #.astype(float)\n",
        "    xarr = np.array(xarr)\n",
        "    bbox = xarr.astype(float)\n",
        "\n",
        "    new_coords = coord_rotate(bbox[0], bbox[1], 0.5, 0.5, math.radians(90))    \n",
        "    \n",
        "    bbox[0] = new_coords[0]\n",
        "    bbox[1] = new_coords[1]\n",
        "    xtemp = bbox[2]\n",
        "    bbox[2] = bbox[3]\n",
        "    bbox[3] = xtemp    \n",
        "    \n",
        "    return str([bbox[0], bbox[1], bbox[2], bbox[3]])\n",
        "\n",
        "## External code pabloberhauser kaggle\n",
        "def convert(size, box):\n",
        "    \n",
        "    dw = 1. / size[0]\n",
        "    dh = 1. / size[1]\n",
        "    x = (box[0] + box[1]) / 2.0\n",
        "    y = (box[2] + box[3]) / 2.0\n",
        "    w = box[1] - box[0]\n",
        "    h = box[3] - box[2]\n",
        "    x = x * dw\n",
        "    w = w * dw\n",
        "    y = y * dh\n",
        "    h = h * dh\n",
        "    return [x, y, w, h]\n",
        "\n",
        "def convert_to_yolo_label(coco_format_box, w = 1024, h = 1024):\n",
        "\n",
        "    xarr = coco_format_box.strip('[]').split(\",\") #.astype(float)\n",
        "    xarr = np.array(xarr)\n",
        "    bbox = xarr.astype(float)\n",
        "    \n",
        "    xmin = bbox[0]\n",
        "    xmax = bbox[0] + bbox[2]\n",
        "    ymin = bbox[1]\n",
        "    ymax = bbox[1] + bbox[3]\n",
        "    b = (float(xmin), float(xmax), float(ymin), float(ymax))\n",
        "    yolo_box = convert((w, h), b)\n",
        "  \n",
        "    # Sanity check on calculations - Take this opportunity to check that conversion works\n",
        "    if np.max(yolo_box) > 1 or np.min(yolo_box) < 0: \n",
        "        print(\"BOX HAS AN ISSUE\")\n",
        "        \n",
        "    return str(yolo_box)\n",
        "\n",
        "# convert the bbox to the darknet format\n",
        "boxes_df['class'] = 0\n",
        "boxes_df['bbox_cords'] = 0\n",
        "boxes_df['yolo_box'] = boxes_df.bbox.apply(convert_to_yolo_label)\n",
        "boxes_df['yolo_box_i'] = boxes_df.yolo_box.apply(calculate_inverted)\n",
        "boxes_df['yolo_box_r'] = boxes_df.yolo_box.apply(calculate_rotated)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MIkMgclsZgh2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "5ff65bc6-c2ef-4b48-d275-d09d5bb23203"
      },
      "source": [
        "print(boxes_df.head(3))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    image_id  ...                                         yolo_box_r\n",
            "0  b6ab77fd7  ...     [0.765625, 0.841796875, 0.03515625, 0.0546875]\n",
            "1  b6ab77fd7  ...  [0.4365234375, 0.2841796875, 0.056640625, 0.12...\n",
            "2  b6ab77fd7  ...     [0.4296875, 0.404296875, 0.15625, 0.072265625]\n",
            "\n",
            "[3 rows x 10 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dCaTiGmIZjGM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "076bb50a-23e6-4d01-9301-8fde621c30f1"
      },
      "source": [
        "print(\"We have {} unique images with boxes.\".format(len(boxes_df.image_id.unique())))\n",
        "unique_img_ids = boxes_df.image_id.unique()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "We have 3373 unique images with boxes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oe6JC5TkZlXW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# write the label files in the format that Yolo expects - darknet format\n",
        "images_path = \"images/train/\"\n",
        "labels_path = \"labels/train/\"\n",
        "\n",
        "def delete_txtlabels ():\n",
        "\n",
        "    image_names = boxes_df['image_id'].unique()\n",
        "    for name in image_names:\n",
        "        fpath = labels_path + name + '-0.txt'    \n",
        "        os.system(\"rm \" + fpath)\n",
        "\n",
        "    return\n",
        "\n",
        "#delete_txtlabels()\n",
        "\n",
        "def write_bboxfiles ():\n",
        "\n",
        "    image_names = boxes_df['image_id'].unique()\n",
        "    \n",
        "    img_ext = ['-0','-r','-i'] #\n",
        "    src_col = ['yolo_box','yolo_box_r','yolo_box_i'] # \n",
        "    \n",
        "    full_cols = ['class', 'x_center', 'y_center', 'bb_width', 'bb_height']\n",
        "    expanded_cols = ['x_center', 'y_center', 'bb_width', 'bb_height']\n",
        "    \n",
        "    for i in range(len(img_ext)):        \n",
        "                \n",
        "        boxes_df['bbox_cords'] = boxes_df[src_col[i]].str.strip('[]')\n",
        "        boxes_df[expanded_cols] = boxes_df['bbox_cords'].str.split(\",\", expand=True).astype(float)\n",
        "        \n",
        "        for name in image_names:\n",
        "\n",
        "            fpath = labels_path  + name + img_ext[i] + '.txt'\n",
        "            image = images_path + name + img_ext[i] + '.jpg'\n",
        "\n",
        "            if (os.path.isfile(image)):\n",
        "                # get the rows in the dataframe that correspond to this image\n",
        "                temp_df = boxes_df[boxes_df['image_id'] == name]\n",
        "                temp_df[full_cols].to_csv(fpath, header=None, index=None, sep=' ')                    \n",
        "                \n",
        "    return \n",
        "\n",
        "write_bboxfiles ()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TETDyUplZnJe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b251b6d1-f5e7-425a-f45f-50da9fa02616"
      },
      "source": [
        "def getimages_wnolabels ():\n",
        "    \n",
        "    all_imgs = glob.glob(\"images/train/*-0.jpg\")\n",
        "    all_imgs = [i.split(\"/\")[-1].replace(\"-0.jpg\", \"\") for i in all_imgs]\n",
        "\n",
        "    positive_imgs = boxes_df.image_id.unique()\n",
        "\n",
        "    img_list = set(all_imgs)\n",
        "\n",
        "    # find all of the images without labels\n",
        "    img_list.difference_update(positive_imgs)\n",
        "    print(len(all_imgs), len(positive_imgs), len(img_list))\n",
        "\n",
        "    nolabel_images = list(img_list)\n",
        "    \n",
        "    return nolabel_images\n",
        "\n",
        "def create_emptyLabels():\n",
        "    \n",
        "    negative_images = getimages_wnolabels ()\n",
        "    \n",
        "    img_ext = ['0','r','i'] #    \n",
        "\n",
        "    for i in range(len(img_ext)): \n",
        "        \n",
        "        # write an empty label file for images with no bboxes\n",
        "        for image in list(negative_images):            \n",
        "            fname = \"{}-\".format(image) + img_ext[i] + \".txt\"\n",
        "            fpath = \"labels/train/\"\n",
        "            with open(os.path.join( fpath, fname), 'w') as fp: \n",
        "                pass\n",
        "    \n",
        "    return \n",
        "\n",
        "create_emptyLabels()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "3422 3373 49\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ug_0--JHZo2-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "1553080c-3e0a-4525-acce-ce835760ccd3"
      },
      "source": [
        "# Split data randomly into training and test set\n",
        "def split_train_testset ():\n",
        "    n = 610    \n",
        "\n",
        "    all_imgs = glob.glob(\"images/train/*-0.jpg\")\n",
        "    all_imgs = [i.split(\"/\")[-1].replace(\".jpg\", \"\") for i in all_imgs]\n",
        "    np.random.shuffle(all_imgs)\n",
        "    test_set = all_imgs[-n:]\n",
        "\n",
        "    for image_labels in test_set:\n",
        "        shutil.move(\"images/train/{}.jpg\".format(image_labels), 'images/val/')\n",
        "        shutil.move(\"labels/train/{}.txt\".format(image_labels), 'labels/val/')\n",
        "        \n",
        "    return True\n",
        "\n",
        "split_train_testset ()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rTyeSvDnZrFe",
        "colab_type": "text"
      },
      "source": [
        "Train Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQn19P1TBVKm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "outputId": "beb2356c-f2e5-4bea-9230-7e8c2ac95965"
      },
      "source": [
        "%cd ../yolov5\n",
        "!ls"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/yolov5\n",
            "data\t    labels.png\t requirements.txt    test_batch0_pred.jpg  weights\n",
            "detect.py   LICENSE\t results.png\t     test.py\n",
            "Dockerfile  models\t results.txt\t     train.py\n",
            "hubconf.py  __pycache__  runs\t\t     tutorial.ipynb\n",
            "inference   README.md\t test_batch0_gt.jpg  utils\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ke-LO_x0BZT0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "816f09c6-94e5-4946-fea7-e3f31a677bf6"
      },
      "source": [
        "!python train.py --img 512 --batch 24 --epochs 10 --data ./data/wheat.yaml --cfg ./models/yolov5l.yaml --weights ./weights/yolov5l.pt --single-cls --resume"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Apex recommended for faster mixed precision training: https://github.com/NVIDIA/apex\n",
            "{'lr0': 0.01, 'momentum': 0.937, 'weight_decay': 0.0005, 'giou': 0.05, 'cls': 0.58, 'cls_pw': 1.0, 'obj': 1.0, 'obj_pw': 1.0, 'iou_t': 0.2, 'anchor_t': 4.0, 'fl_gamma': 0.0, 'hsv_h': 0.014, 'hsv_s': 0.68, 'hsv_v': 0.36, 'degrees': 0.0, 'translate': 0.0, 'scale': 0.5, 'shear': 0.0}\n",
            "Namespace(adam=False, batch_size=24, bucket='', cache_images=False, cfg='./models/yolov5l.yaml', data='./data/wheat.yaml', device='', epochs=10, evolve=False, img_size=[512], multi_scale=False, name='', noautoanchor=False, nosave=False, notest=False, rect=False, resume=True, single_cls=True, weights='weights/last.pt')\n",
            "Using CUDA device0 _CudaDeviceProperties(name='Tesla T4', total_memory=15079MB)\n",
            "\n",
            "2020-06-28 04:40:39.833196: I tensorflow/stream_executor/platform/default/dso_loader.cc:44] Successfully opened dynamic library libcudart.so.10.1\n",
            "Start Tensorboard with \"tensorboard --logdir=runs\", view at http://localhost:6006/\n",
            "\n",
            "              from  n    params  module                                  arguments                     \n",
            "  0             -1  1      7040  models.common.Focus                     [3, 64, 3]                    \n",
            "  1             -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  2             -1  1    161152  models.common.BottleneckCSP             [128, 128, 3]                 \n",
            "  3             -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  4             -1  1   1627904  models.common.BottleneckCSP             [256, 256, 9]                 \n",
            "  5             -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
            "  6             -1  1   6499840  models.common.BottleneckCSP             [512, 512, 9]                 \n",
            "  7             -1  1   4720640  models.common.Conv                      [512, 1024, 3, 2]             \n",
            "  8             -1  1   2624512  models.common.SPP                       [1024, 1024, [5, 9, 13]]      \n",
            "  9             -1  1  10234880  models.common.BottleneckCSP             [1024, 1024, 3, False]        \n",
            " 10             -1  1    525312  models.common.Conv                      [1024, 512, 1, 1]             \n",
            " 11             -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12        [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13             -1  1   2823680  models.common.BottleneckCSP             [1024, 512, 3, False]         \n",
            " 14             -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
            " 15             -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16        [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17             -1  1    707328  models.common.BottleneckCSP             [512, 256, 3, False]          \n",
            " 18             -1  1      4626  torch.nn.modules.conv.Conv2d            [256, 18, 1, 1]               \n",
            " 19             -2  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
            " 20       [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 21             -1  1   2561536  models.common.BottleneckCSP             [512, 512, 3, False]          \n",
            " 22             -1  1      9234  torch.nn.modules.conv.Conv2d            [512, 18, 1, 1]               \n",
            " 23             -2  1   2360320  models.common.Conv                      [512, 512, 3, 2]              \n",
            " 24       [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 25             -1  1  10234880  models.common.BottleneckCSP             [1024, 1024, 3, False]        \n",
            " 26             -1  1     18450  torch.nn.modules.conv.Conv2d            [1024, 18, 1, 1]              \n",
            " 27   [-1, 22, 18]  1         0  models.yolo.Detect                      [1, [[42, 56, 50, 90, 90, 57], [33, 34, 45, 39, 65, 35], [23, 22, 43, 22, 23, 44]]]\n",
            "Model Summary: 335 layers, 4.73933e+07 parameters, 4.73933e+07 gradients\n",
            "\n",
            "Optimizer groups: 110 .bias, 118 conv.weight, 107 other\n",
            "Caching labels ../wheat/labels/train.npy (9520 found, 0 missing, 136 empty, 0 duplicate, for 9656 images): 100% 9656/9656 [00:00<00:00, 14706.95it/s]\n",
            "Caching labels ../wheat/labels/val (599 found, 0 missing, 11 empty, 0 duplicate, for 610 images): 100% 610/610 [00:00<00:00, 4973.86it/s]\n",
            "\n",
            "Analyzing anchors... Best Possible Recall (BPR) = 0.9990\n",
            "Image sizes 512 train, 512 test\n",
            "Using 2 dataloader workers\n",
            "Starting training for 10 epochs...\n",
            "\n",
            "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n",
            "       5/9     10.2G   0.04956    0.2731         0    0.3227       411       512: 100% 403/403 [09:52<00:00,  1.47s/it]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 26/26 [00:54<00:00,  2.10s/it]\n",
            "                 all         610    2.62e+04       0.534       0.928       0.901        0.38\n",
            "\n",
            "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n",
            "       6/9     13.5G   0.04479    0.2628         0    0.3076       469       512: 100% 403/403 [09:50<00:00,  1.46s/it]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 26/26 [00:47<00:00,  1.84s/it]\n",
            "                 all         610    2.62e+04       0.605       0.951       0.938       0.501\n",
            "\n",
            "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n",
            "       7/9     13.5G   0.04381    0.2607         0    0.3045       422       512: 100% 403/403 [09:50<00:00,  1.47s/it]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 26/26 [00:47<00:00,  1.82s/it]\n",
            "                 all         610    2.62e+04       0.647       0.948       0.941       0.511\n",
            "\n",
            "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n",
            "       8/9     13.5G   0.04334    0.2582         0    0.3015       434       512: 100% 403/403 [09:50<00:00,  1.47s/it]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 26/26 [00:47<00:00,  1.83s/it]\n",
            "                 all         610    2.62e+04       0.645       0.951       0.943       0.518\n",
            "\n",
            "     Epoch   gpu_mem      GIoU       obj       cls     total   targets  img_size\n",
            "       9/9     13.5G   0.04327    0.2586         0    0.3018       422       512: 100% 403/403 [09:50<00:00,  1.47s/it]\n",
            "               Class      Images     Targets           P           R      mAP@.5  mAP@.5:.95: 100% 26/26 [00:48<00:00,  1.87s/it]\n",
            "                 all         610    2.62e+04       0.662       0.952       0.945       0.526\n",
            "5 epochs completed in 0.893 hours.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_r1DlaT8lu-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "80d21619-e62c-49ee-ed51-db0d11148372"
      },
      "source": [
        "!python detect.py --img 512 --iou-thres 0.7 --weights weights/best.pt --source ../wheat/images/test  --save-txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(agnostic_nms=False, augment=False, classes=None, conf_thres=0.4, device='', fourcc='mp4v', img_size=512, iou_thres=0.6, output='inference/output', save_txt=True, source='../wheat/images/test', view_img=False, weights='weights/best.pt')\n",
            "Using CUDA device0 _CudaDeviceProperties(name='Tesla T4', total_memory=15079MB)\n",
            "\n",
            "image 1/10 ../wheat/images/test/2fd875eaa.jpg: 512x512 27 wheats, Done. (0.025s)\n",
            "image 2/10 ../wheat/images/test/348a992bb.jpg: 512x512 37 wheats, Done. (0.027s)\n",
            "image 3/10 ../wheat/images/test/51b3e36ab.jpg: 512x512 25 wheats, Done. (0.025s)\n",
            "image 4/10 ../wheat/images/test/51f1be19e.jpg: 512x512 18 wheats, Done. (0.024s)\n",
            "image 5/10 ../wheat/images/test/53f253011.jpg: 512x512 29 wheats, Done. (0.027s)\n",
            "image 6/10 ../wheat/images/test/796707dd7.jpg: 512x512 26 wheats, Done. (0.024s)\n",
            "image 7/10 ../wheat/images/test/aac893a91.jpg: 512x512 20 wheats, Done. (0.021s)\n",
            "image 8/10 ../wheat/images/test/cb8d261a3.jpg: 512x512 26 wheats, Done. (0.021s)\n",
            "image 9/10 ../wheat/images/test/cc3532ff6.jpg: 512x512 25 wheats, Done. (0.022s)\n",
            "image 10/10 ../wheat/images/test/f5a1f0358.jpg: 512x512 27 wheats, Done. (0.021s)\n",
            "Results saved to /content/yolov5/inference/output\n",
            "Done. (0.571s)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PQtALK6hsI63",
        "colab_type": "text"
      },
      "source": [
        "Save Model to Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mMrazTP_S9iJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "65c2f64e-c4ed-496d-f806-06df01bf63ba"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GXbeSeRXsIHH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cp /content/yolov5/weights/best.pt  \"/content/gdrive/My Drive/Colab Notebooks/WheatDetection/\"\n",
        "#%cp /content/yolov5/weights/last.pt  \"/content/gdrive/My Drive/Colab Notebooks/WheatDetection/\""
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AyORgKwGvkcI",
        "colab_type": "text"
      },
      "source": [
        "Opt Reload Model from Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nn0L6Q2Fsx-W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%cp \"/content/gdrive/My Drive/Colab Notebooks/WheatDetection/last.pt\" /content/yolov5/weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55HLYwsctbUe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "e00a0461-5ec7-478e-8337-4fd8d50009a5"
      },
      "source": [
        "%cd .."
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7-MdrzntPHE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "shutil.make_archive('output', 'zip', 'inference/output')\n",
        "#shutil.make_archive('wheat-processed', 'zip', 'wheat')\n",
        "#%cp /content/wheat-processed.zip \"/content/gdrive/My Drive/Colab Notebooks/WheatDetection/\""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}